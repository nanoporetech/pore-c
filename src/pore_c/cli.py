import logging
import os.path
import pandas as pd

import click
import click_log

logger = logging.getLogger(__name__)
click_log.basic_config(logger)


class NaturalOrderGroup(click.Group):
    """Command group trying to list subcommands in the order they were added.
    """

    def list_commands(self, ctx):
        """List command names as they are in commands dict.
        """
        return self.commands.keys()


@click.group(cls=NaturalOrderGroup)
@click_log.simple_verbosity_option(logger)
def cli():
    """Pore-C tools"""
    pass


@cli.command(short_help="Virtual digest of reference genome.")
@click.argument("reference_fasta", type=click.Path(exists=True))
@click.argument("restriction_pattern")
@click.argument("output_prefix")
def generate_fragments(reference_fasta, restriction_pattern, output_prefix):
    """
    Carry out a virtual digestion of RESTRICTION_PATTERN on REFERENCE_FASTA writing results to BEDFILE and HICREF.

    The RESTRICTION_PATTERN can be specified using either i) the name of a
    restriction enzyme as available in the Biopython Restriction package (eg.
    HindIII, case sensitive) ii) as a fixed with bin prefixed by 'bin:', or
    iii) as a python regular expression prefixed by 'regex:'. Note that the
    positions returned by these two methods will differ as the biopython method
    will return fragments delimited by the actual cut site, whereas the regular
    expression will return the positions of the recognition patterns.

    Some sample RESTRICTION_PATTERNs:

    \b
      - an enzyme name, note case sensitive: "HindIII"
      - a fixed width bin: "bin:50k"
      - a single site (HindIII): "regex:AAGCTT"
      - two sites (ecoRI and NotI): "regex:(GAATTC|GCGGCCGC)"
      - degenerate site (BglI): "regex:GCCNNNNNGGC"
      - degenerate site (ApoI): "regex:RAATY"

    The BEDFILE contains an entry for each of the fragments generated by the by the digest.

    \b
    chr\tstart\tend\tfragment_id
    \b

    The HICREF follows the Juicer format generated by this script:
    (https://github.com/aidenlab/juicer/blob/master/misc/generate_site_positions.py):

    \b
    chr site1 site2 site3 ... chrLength
    \b
    as in:
    1 11160 12411 12461 ... 249250621
    2 11514 11874 12160 ... 243199373
    3 60138 60662 60788 ... 198022430

    """
    from pore_c.analyses.reference import create_fragment_map
    import dask.dataframe as dd
    from pathlib import Path
    logger.info(
        "Creating fragments from reference genome: {} and digestion pattern {}".format(
            reference_fasta, restriction_pattern
        )
    )
    faidx_file = reference_fasta + ".fai"
    if not os.path.exists(faidx_file):
        raise IOError(
            "Faidx file doesn't exist, please run 'samtools faidx {}'".format(
                reference_fasta
            )
        )
    output_prefix = Path(output_prefix)
    cat = create_fragment_map(reference_fasta, restriction_pattern, output_prefix)
    logger.debug("Created fragment map catalog: {}".format(cat.path))


@cli.command(short_help="Filter alignments")
@click.argument("input_bam", type=click.Path(exists=True))
@click.argument("output_prefix")
@click.option(
    "--save_fail_bam",
    is_flag=True,
    help="Save alignments that fail filter to {prefix}.pass.bam"
)
@click.option(
    "--save_align_table",
    is_flag=True,
    help="Save the filter status of each alignment to {prefix}.alignment.parquet"
)
@click.option(
    "--save_read_table",
    is_flag=True,
    help="Save read-level statistics to {prefix}.read.parquet"
)
def filter_alignments(input_bam, output_prefix, save_fail_bam, save_align_table, save_read_table):
    from pore_c.analyses.alignments import filter_alignments as filt
    from pathlib import Path

    output_prefix = Path(output_prefix)

    pass_bam = output_prefix.with_suffix(".pass.bam")
    if not pass_bam.parent.exists():
        pass_bam.parent.mkdir()
    fail_bam = output_prefix.with_suffix(".fail.bam") if save_fail_bam else None
    align_table = output_prefix.with_suffix(".alignment.parquet") if save_align_table else None
    read_table = output_prefix.with_suffix(".read.parquet") if save_read_table else None
    catalog_file = output_prefix.with_suffix(".catalog.yaml")

    fail = False
    for outfile in [pass_bam, fail_bam, align_table, read_table]:
        if outfile is not None and outfile.exists():
            logger.error("Output file already exists: {}".format(outfile))
            fail = True

    if fail:
        raise click.ClickException("An error was encountered while setting up alignment filters")

    res = filt(input_bam, pass_bam=pass_bam, fail_bam=fail_bam, align_table=align_table, read_table=read_table, catalog_file = catalog_file)
    logger.info(res)


@cli.command(
    short_help="create a .poreC file from a namesorted alignment of poreC data"
)
@click.argument(
    "filter_catalog", type=click.Path(exists=True)
)
@click.argument(
    "fragment_bed_file", type=click.Path(exists=True)
)
def map_to_fragments(filter_catalog, fragment_bed_file):
    from intake import open_catalog
    from pore_c.datasources import IndexedBedFile
    from ncls import NCLS

    catalog = open_catalog(filter_catalog)

    bf = IndexedBedFile(fragment_bed_file, metadata={})
    fragments = bf.read().astype({'name': int}).set_index("name", drop=False)
    print(fragments[fragments['name'].diff() != 1.0])
    assert(fragments.index.is_monotonic_increasing)
    fragment_intervals = {}
    for chrom, chrom_df in bf.read().groupby("chrom"):
        fragment_intervals[chrom] = NCLS(chrom_df.start.values, chrom_df.end.values, chrom_df['name'].astype(int).values)

    for chunk in catalog.align_table.read_chunked():
        print(chunk.head())
        for chrom, chrom_df in chunk.groupby("chrom"):
            if chrom in fragment_intervals:
                frag_indices, chunk_indices = fragment_intervals[chrom].all_overlaps_both(
                    chrom_df.start.astype(int).values,
                    chrom_df.end.astype(int).values,
                    chrom_df.index.astype(int).values
                )
                overlap_df = pd.DataFrame({
                    'hit_idx': chunk_indices,
                    'hit_start': chunk['start'].take(chunk_indices),
                    'hit_end': chunk['end'].take(chunk_indices),
                    'frag_idx': frag_indices,
                    'frag_start': fragments['start'].take(frag_indices),
                    'frag_end': fragments['end'].take(frag_indices)
                })
                print(overlap_df.head())
        break
    raise ValueError(catalog)
    map_to_fragments_tool(input_bed, fragment_bed_file, output_porec, method, stats)
